{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "if sys.version_info.major == 2:\n",
    "    import Tkinter as tk\n",
    "else:\n",
    "    import tkinter as tk\n",
    "\n",
    "UNIT = 40   # pixels\n",
    "MAZE_H = 4  # grid height\n",
    "MAZE_W = 4  # grid width    \n",
    "\n",
    "class Maze(tk.Tk, object):\n",
    "    def __init__(self):\n",
    "        super(Maze, self).__init__()\n",
    "        self.action_space = ['u', 'd', 'l', 'r']\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.n_features = 2\n",
    "        self.title('maze')\n",
    "        self.geometry('{0}x{1}'.format(MAZE_H * UNIT, MAZE_H * UNIT))\n",
    "        self._build_maze()\n",
    "\n",
    "    def _build_maze(self):\n",
    "        self.canvas = tk.Canvas(self, bg='white',\n",
    "                           height=MAZE_H * UNIT,\n",
    "                           width=MAZE_W * UNIT)\n",
    "\n",
    "        # create grids\n",
    "        for c in range(0, MAZE_W * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = c, 0, c, MAZE_H * UNIT\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "        for r in range(0, MAZE_H * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = 0, r, MAZE_W * UNIT, r\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "\n",
    "        # create origin\n",
    "        origin = np.array([20, 20])\n",
    "\n",
    "        # hell\n",
    "        hell1_center = origin + np.array([UNIT * 2, UNIT])\n",
    "        self.hell1 = self.canvas.create_rectangle(\n",
    "            hell1_center[0] - 15, hell1_center[1] - 15,\n",
    "            hell1_center[0] + 15, hell1_center[1] + 15,\n",
    "            fill='black')\n",
    "        # hell\n",
    "        # hell2_center = origin + np.array([UNIT, UNIT * 2])\n",
    "        # self.hell2 = self.canvas.create_rectangle(\n",
    "        #     hell2_center[0] - 15, hell2_center[1] - 15,\n",
    "        #     hell2_center[0] + 15, hell2_center[1] + 15,\n",
    "        #     fill='black')\n",
    "\n",
    "        # create oval\n",
    "        oval_center = origin + UNIT * 2\n",
    "        self.oval = self.canvas.create_oval(\n",
    "            oval_center[0] - 15, oval_center[1] - 15,\n",
    "            oval_center[0] + 15, oval_center[1] + 15,\n",
    "            fill='yellow')\n",
    "\n",
    "        # create red rect\n",
    "        self.rect = self.canvas.create_rectangle(\n",
    "            origin[0] - 15, origin[1] - 15,\n",
    "            origin[0] + 15, origin[1] + 15,\n",
    "            fill='red')\n",
    "\n",
    "        # pack all\n",
    "        self.canvas.pack()\n",
    "\n",
    "    def reset(self):\n",
    "        self.update()\n",
    "        time.sleep(0.1)\n",
    "        self.canvas.delete(self.rect)\n",
    "        origin = np.array([20, 20])\n",
    "        self.rect = self.canvas.create_rectangle(\n",
    "            origin[0] - 15, origin[1] - 15,\n",
    "            origin[0] + 15, origin[1] + 15,\n",
    "            fill='red')\n",
    "        # return observation\n",
    "        return (np.array(self.canvas.coords(self.rect)[:2]) - np.array(self.canvas.coords(self.oval)[:2]))/(MAZE_H*UNIT)\n",
    "\n",
    "    def step(self, action):\n",
    "        s = self.canvas.coords(self.rect)\n",
    "        base_action = np.array([0, 0])\n",
    "        if action == 0:   # up\n",
    "            if s[1] > UNIT:\n",
    "                base_action[1] -= UNIT\n",
    "        elif action == 1:   # down\n",
    "            if s[1] < (MAZE_H - 1) * UNIT:\n",
    "                base_action[1] += UNIT\n",
    "        elif action == 2:   # right\n",
    "            if s[0] < (MAZE_W - 1) * UNIT:\n",
    "                base_action[0] += UNIT\n",
    "        elif action == 3:   # left\n",
    "            if s[0] > UNIT:\n",
    "                base_action[0] -= UNIT\n",
    "\n",
    "        self.canvas.move(self.rect, base_action[0], base_action[1])  # move agent\n",
    "\n",
    "        next_coords = self.canvas.coords(self.rect)  # next state\n",
    "\n",
    "        # reward function\n",
    "        if next_coords == self.canvas.coords(self.oval):\n",
    "            reward = 1\n",
    "            done = True\n",
    "        elif next_coords in [self.canvas.coords(self.hell1)]:\n",
    "            reward = -1\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "        s_ = (np.array(next_coords[:2]) - np.array(self.canvas.coords(self.oval)[:2]))/(MAZE_H*UNIT)\n",
    "        return s_, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        # time.sleep(0.01)\n",
    "        self.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class DeepQNetwork:\n",
    "    # 建立神经网络\n",
    "    def _build_net(self):\n",
    "        # 用来接收observation\n",
    "        self.s = tf.placeholder(tf.float32, \n",
    "            [None, self.n_features], name='s')\n",
    "        # 用来接收q_target的值\n",
    "        self.q_target = tf.placeholder(tf.float32, \n",
    "            [None, self.n_actions], name='Q_target')\n",
    "        with tf.variable_scope('eval_net'):\n",
    "            # c_names(collections_names) 是在更新target_net时用到\n",
    "            c_names = ['eval_net_params', \n",
    "                       tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "            n_l1 = 10\n",
    "            w_initializer = tf.random_normal_initializer(0., 0.3)\n",
    "            b_initializer = tf.constant_initializer(0.1)\n",
    "        \n",
    "            # eval_net的第一层\n",
    "            # collections是在更新target_net参数时会用到\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1],\n",
    "                    initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1],\n",
    "                    initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)\n",
    "            \n",
    "            # eval_net的第二层\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions],\n",
    "                    initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions],\n",
    "                    initializer=b_initializer, collections=c_names)\n",
    "                self.q_eval = tf.matmul(l1, w2) + b2\n",
    "        \n",
    "        with tf.variable_scope('loss'): # 求误差\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(\n",
    "                self.q_target, self.q_eval))\n",
    "        with tf.variable_scope('train'): # 梯度下降\n",
    "            self._train_op = tf.train.RMSPropOptimizer(\n",
    "                self.lr).minimize(self.loss)\n",
    "            \n",
    "        # --------- 创建target神经网络，提供target Q--------\n",
    "        self.s_ = tf.placeholder(tf.float32, [None, \n",
    "            self.n_features], name='s_') # 接收下一个observation\n",
    "        with tf.variable_scope('target_net'):\n",
    "            # c_names(collections_names)\n",
    "            # 是在更新 target_net 参数时会用到\n",
    "            c_names = ['target_net_params', \n",
    "                       tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "            \n",
    "            # target_net 的第一层. collections \n",
    "            #是在更新 target_net 参数时会用到\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1],\n",
    "                    initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1],\n",
    "                    initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)\n",
    "            # target_net 的第二层. collections \n",
    "            # 是在更新 target_net 参数时会用到\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions],\n",
    "                    initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions],\n",
    "                    initializer=b_initializer, collections=c_names)\n",
    "                self.q_next = tf.matmul(l1, w2) + b2\n",
    "    \n",
    "    # 初始值\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions,\n",
    "        n_features,\n",
    "        learning_rate=0.01,\n",
    "        reward_decay=0.9,\n",
    "        e_greedy=0.9,\n",
    "        replace_target_iter=300,\n",
    "        memory_size=500,\n",
    "        batch_size=32,\n",
    "        e_greedy_increment=None,\n",
    "        output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy # epsilon的最大值\n",
    "        # 更换target_net的步数\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size # 记忆上限\n",
    "        self.batch_size = batch_size # 每次更新提取多少记忆\n",
    "        self.epsilon_increment = e_greedy_increment # epsilon的增量\n",
    "        # 是否开启探索模式\n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "        \n",
    "        # 记录学习次数（用于判断是否更换target_net参数）\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        # 初始化全0记忆[s, a, r, s_]\n",
    "        self.memory = np.zeros((self.memory_size, n_features*2+2))\n",
    "        \n",
    "        # 创建[target_net, evaluate_net]\n",
    "        self._build_net()\n",
    "        \n",
    "        # 替换 target net 的参数\n",
    "        # 提取 target_net 的参数\n",
    "        t_params = tf.get_collection('target_net_params')\n",
    "        # 提取 eval_net 的参数\n",
    "        e_params = tf.get_collection('eval_net_params')\n",
    "        # API待查\n",
    "        self.replace_target_op = [tf.assign(t, e) for t,e in zip(\n",
    "            t_params, e_params)] # 更新 target_net 参数\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        if output_graph:\n",
    "            tf.summary.FileWriter('logs/', self.sess.graph)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.cost_his = [] # 记录所有cost变化\n",
    "    \n",
    "    # 存储记忆\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "        \n",
    "        # 记录一条[s,a,r,s_]记录\n",
    "        transition = np.hstack((s, [a,r], s_))\n",
    "        \n",
    "        # 总memory大小固定，若超出大小，替换旧memory\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    # 选行为\n",
    "    def choose_action(self, observation):\n",
    "        # 统一observation的shape(1, size_of_observation)\n",
    "        observation = observation[np.newaxis, :]\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # 让 eval_net 生成所有actions的值，并选择值最大的action\n",
    "            actions_value = self.sess.run(self.q_eval, feed_dict={\n",
    "                self.s: observation\n",
    "            })\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            # 随机选择\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return action\n",
    "    \n",
    "    # 学习\n",
    "    def learn(self):\n",
    "        # 检查是否替换 target_net 参数\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.sess.run(self.replace_target_op)\n",
    "            print('\\ntarget_params_replaced\\n')\n",
    "            \n",
    "        # 从 memory 中随机抽取 batch_size 分记忆\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, \n",
    "                size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter,\n",
    "                size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "        \n",
    "        # 获取 q_next 和 q_eval\n",
    "        q_next, q_eval = self.sess.run(\n",
    "            [self.q_next, self.q_eval],\n",
    "            feed_dict={\n",
    "                self.s_: batch_memory[:, -self.n_features:],\n",
    "                self.s: batch_memory[:, :self.n_features]\n",
    "            })\n",
    "        \n",
    "        # 下面这几步十分重要. q_next, q_eval 包含所有 action 的值,\n",
    "        # 而我们需要的只是已经选择好的 action 的值, 其他的并不需要.\n",
    "        # 所以我们将其他的 action 值全变成 0, 将用到的 action 误差值 反向传递回去, 作为更新凭据.\n",
    "        # 这是我们最终要达到的样子, 比如 q_target - q_eval = [1, 0, 0] - [-1, 0, 0] = [2, 0, 0]\n",
    "        # q_eval = [-1, 0, 0] 表示这一个记忆中有我选用过 action 0, 而 action 0 带来的 Q(s, a0) = -1, 所以其他的 Q(s, a1) = Q(s, a2) = 0.\n",
    "        # q_target = [1, 0, 0] 表示这个记忆中的 r+gamma*maxQ(s_) = 1, 而且不管在 s_ 上我们取了哪个 action,\n",
    "        # 我们都需要对应上 q_eval 中的 action 位置, 所以就将 1 放在了 action 0 的位置.\n",
    "\n",
    "        # 下面也是为了达到上面说的目的, 不过为了更方面让程序运算, 达到目的的过程有点不同.\n",
    "        # 是将 q_eval 全部赋值给 q_target, 这时 q_target-q_eval 全为 0,\n",
    "        # 不过 我们再根据 batch_memory 当中的 action 这个 column 来给 q_target 中的对应的 memory-action 位置来修改赋值.\n",
    "        # 使新的赋值为 reward + gamma * maxQ(s_), 这样 q_target-q_eval 就可以变成我们所需的样子.\n",
    "        # 具体在下面还有一个举例说明.\n",
    "        \n",
    "        q_target = q_eval.copy()\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.n_features].astype(int)\n",
    "        reward = batch_memory[:, self.n_features + 1]\n",
    "        \n",
    "        q_target[batch_index, eval_act_index] = \\\n",
    "        reward + self.gamma*np.max(q_next, axis=1)\n",
    "        \n",
    "        \"\"\"\n",
    "        假如在这个 batch 中, 我们有2个提取的记忆, 根据每个记忆可以生产3个 action 的值:\n",
    "        q_eval =\n",
    "        [[1, 2, 3],\n",
    "         [4, 5, 6]]\n",
    "\n",
    "        q_target = q_eval =\n",
    "        [[1, 2, 3],\n",
    "         [4, 5, 6]]\n",
    "\n",
    "        然后根据 memory 当中的具体 action 位置来修改 q_target 对应 action 上的值:\n",
    "        比如在:\n",
    "            记忆 0 的 q_target 计算值是 -1, 而且我用了 action 0;\n",
    "            记忆 1 的 q_target 计算值是 -2, 而且我用了 action 2:\n",
    "        q_target =\n",
    "        [[-1, 2, 3],\n",
    "         [4, 5, -2]]\n",
    "\n",
    "        所以 (q_target - q_eval) 就变成了:\n",
    "        [[(-1)-(1), 0, 0],\n",
    "         [0, 0, (-2)-(6)]]\n",
    "\n",
    "        最后我们将这个 (q_target - q_eval) 当成误差, 反向传递会神经网络.\n",
    "        所有为 0 的 action 值是当时没有选择的 action, 之前有选择的 action 才有不为0的值.\n",
    "        我们只反向传递之前选择的 action 的值,\n",
    "        \"\"\"\n",
    "        # 训练 eval_net\n",
    "        _, self.cost = self.sess.run([self._train_op, self.loss],\n",
    "            feed_dict={self.s: batch_memory[:, :self.n_features],\n",
    "            self.q_target: q_target})\n",
    "        self.cost_his.append(self.cost) # 记录cost误差\n",
    "        \n",
    "        # 逐渐增加 epsilon，降低行为的随机性\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n",
    "    \n",
    "    # 看看学习效果（可选）\n",
    "    def plot_cost(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(np.arange(len(self.cost_his)), self.cost_his)\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('training steps')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_maze():\n",
    "    step = 0 # 用来控制什么时候学习\n",
    "    for episode in range(300):\n",
    "        # 初始化环境\n",
    "        observation = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            # 刷新环境\n",
    "            env.render()\n",
    "            \n",
    "            # DQN根据观测值选择行为\n",
    "            action = RL.choose_action(observation)\n",
    "            \n",
    "            # 环境根据行为给出下一个state,reward,done\n",
    "            observation_, reward, done = env.step(action)\n",
    "            \n",
    "            # DQN存储记忆\n",
    "            RL.store_transition(observation, action,\n",
    "                reward, observation_)\n",
    "            \n",
    "            # 控制学习起始时间和频率\n",
    "            # 先累积记忆再学习\n",
    "            if (step > 200) and (step % 5 == 0):\n",
    "                RL.learn()\n",
    "                \n",
    "            # 进入下一个state\n",
    "            observation = observation_\n",
    "            \n",
    "            # 如果终止，跳出循环\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            step += 1\n",
    "    \n",
    "    # end of game\n",
    "    print('game over')\n",
    "    env.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 16:10:35.348256  9748 deprecation.py:506] From D:\\Anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "game over\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = Maze()\n",
    "    RL = DeepQNetwork(env.n_actions, env.n_features,\n",
    "            learning_rate=0.01,\n",
    "            reward_decay=0.9,\n",
    "            e_greedy=0.9,\n",
    "            replace_target_iter=200, # 200步替换target_net参数\n",
    "            memory_size=2000, # 记忆上限\n",
    "            # output_graph = True # 是否输出Tensorboard文件\n",
    "            )\n",
    "    env.after(100, run_maze)\n",
    "    env.mainloop()\n",
    "    RL.plot_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
