{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = 6 # 一维世界的宽度\n",
    "ACTIONS= ['left', 'right'] # 探索者的可用动作\n",
    "EPSILON = 0.9 # 贪婪度 greedy\n",
    "ALPHA = 0.1 # 学习率\n",
    "GAMMA = 0.9 # 奖励递减值\n",
    "MAX_EPISODES = 13 # 最大回合数\n",
    "FRESH_TIME = 0.3 # 移动间隔时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_table(n_states, actions):\n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))), # 全0初始化\n",
    "        columns=actions, # columns对应的是行为名称\n",
    "    )\n",
    "    return table\n",
    "\n",
    "def choose_action(state, q_table):\n",
    "    state_actions = q_table.iloc[state, :]\n",
    "    if(np.random.uniform() > EPSILON or (\n",
    "        state_actions.all() == 0)): # 非贪婪 or 该state未探索\n",
    "        action_name = np.random.choice(ACTIONS) # 随机选择\n",
    "    else:\n",
    "        action_name = state_actions.argmax() # 贪婪，选择最大\n",
    "    return action_name\n",
    "       \n",
    "def get_env_feedback(S, A):\n",
    "    if A == 'right':\n",
    "       if S == N_STATES - 2:\n",
    "           S_ = 'terminal'\n",
    "           R = 1\n",
    "       else:\n",
    "           S_ = S + 1\n",
    "           R = 0\n",
    "    else:\n",
    "       R = 0\n",
    "       if S == 0:\n",
    "           S_ = S\n",
    "       else:\n",
    "           S_ = S - 1\n",
    "    return S_, R\n",
    "       \n",
    "def update_env(S, episode, step_counter):\n",
    "    env_list = ['-']*(N_STATES-1) + ['T'] # '------T' 环境\n",
    "    if S == 'terminal':\n",
    "       interaction = 'Episode %s: total_steps = %s' % (\n",
    "           episode + 1, step_counter)\n",
    "       print('\\r{}'.format(interaction), end='')\n",
    "       time.sleep(2)\n",
    "       print('\\r                      ', end='')\n",
    "    else:\n",
    "       env_list[S] = 'o'\n",
    "       interaction = ''.join(env_list)\n",
    "       print('\\r{}'.format(interaction), end='')\n",
    "       time.sleep(FRESH_TIME)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl():\n",
    "    q_table = build_q_table(N_STATES, ACTIONS)\n",
    "    for episode in range(MAX_EPISODES): # 初始化 q table\n",
    "        step_counter = 0\n",
    "        S = 0 # 回合初始位置\n",
    "        is_terminated = False # 是否结束回合\n",
    "        update_env(S, episode, step_counter) # 环境更新\n",
    "        while not is_terminated:\n",
    "            A = choose_action(S, q_table) # 选择行为\n",
    "            S_, R = get_env_feedback(S, A) # 实施行为，得到反馈\n",
    "            q_predict = q_table.loc[S, A] # 估算的（状态-行为）值\n",
    "            if S_ != 'terminal':\n",
    "                # 实际的（状态-行为）值\n",
    "                q_target = R + GAMMA * q_table.iloc[S_, :].max()\n",
    "            else:\n",
    "                q_target = R # s实际的（状态=行为）值\n",
    "                is_terminated = True # terminate this episode\n",
    "                \n",
    "            # q_table 更新\n",
    "            q_table.loc[S,A] += ALPHA * (q_target - q_predict)\n",
    "            S = S_ # 状态更新\n",
    "            \n",
    "            update_env(S, episode, step_counter+1) # 环境更新\n",
    "            \n",
    "            step_counter += 1\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----oT                 = 25"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\mlcc\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      s = 5\n",
      "Q-table: \n",
      "\n",
      "       left     right\n",
      "0  0.000023  0.005042\n",
      "1  0.000005  0.027061\n",
      "2  0.000007  0.111953\n",
      "3  0.000204  0.343331\n",
      "4  0.000810  0.745813\n",
      "5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    q_table = rl()\n",
    "    print('\\r\\nQ-table: \\n')\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
